{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19c25676",
   "metadata": {},
   "source": [
    "## One originator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaef390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import re\n",
    "\n",
    "ORIGINATOR = \"24273\"\n",
    "INPUT = \"./2025-11-13_on_boarding_messages/2025-11-13/com.att.aamg.kafka.prod.sms-prod-eastus2-mt-default.csv\"\n",
    "OUTPUT = f\"./data/input/ref_{ORIGINATOR}.csv\"\n",
    "MAX_ROWS = 1000000\n",
    "DILUTE_FACTOR = 10  # write every Nth matching-originator message\n",
    "\n",
    "def normalize(s):\n",
    "    s = s.strip()\n",
    "    if len(s) >= 2 and s[0] == '\"' and s[-1] == '\"':\n",
    "        s = s[1:-1]\n",
    "    s = s.replace('\"\"', '\"').replace('\\r', ' ').replace('\\n', ' ').replace('\\t', ' ')\n",
    "    return s\n",
    "\n",
    "def get_source_and_fields(raw):\n",
    "    s = normalize(raw)\n",
    "    try:\n",
    "        o = json.loads(s)\n",
    "    except json.JSONDecodeError:\n",
    "        src = re.search(r'\"sourceAddress\"\\s*:\\s*\"([^\"]*)\"', s)\n",
    "        req = re.search(r'\"requestID\"\\s*:\\s*\"([^\"]*)\"', s)\n",
    "        msg = re.search(r'\"shortMessage\"\\s*:\\s*\"([^\"]*)\"', s)\n",
    "        return (src.group(1) if src else None,\n",
    "                req.group(1) if req else None,\n",
    "                msg.group(1).replace('\\\\\"','\"') if msg else None)\n",
    "    return (o.get('sourceAddress'), o.get('requestID'), o.get('shortMessage'))\n",
    "\n",
    "with open(INPUT, newline='', encoding='utf-8') as fin, \\\n",
    "     open(OUTPUT, 'w', newline='', encoding='utf-8') as fout:\n",
    "    reader = csv.DictReader(fin)\n",
    "    writer = csv.DictWriter(fout, fieldnames=['originator_id','message_id','raw_text'])\n",
    "    writer.writeheader()\n",
    "    match_counter = 0\n",
    "    for i, row in enumerate(reader, 1):\n",
    "        src, req, msg = get_source_and_fields(row['Value'])\n",
    "        if src == ORIGINATOR:\n",
    "            match_counter += 1\n",
    "            if match_counter % DILUTE_FACTOR == 0:\n",
    "                writer.writerow({'originator_id': src, 'message_id': req, 'raw_text': msg})\n",
    "        if i % 100000 == 0:\n",
    "            print(f\"Processed {i:,} rows (matches so far: {match_counter:,})\")\n",
    "        if i >= MAX_ROWS:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3617bac8",
   "metadata": {},
   "source": [
    "# loop over a list of originators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18e2947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts: {'61746': 79259, '24273': 43693}\n",
      "Dilute factors: {'24273': 9, '61746': 16}\n",
      "Wrote 24273 -> ./data/input/ref_24273.csv (count=43693, df=9)\n",
      "Wrote 61746 -> ./data/input/ref_61746.csv (count=79259, df=16)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from math import ceil\n",
    "\n",
    "\n",
    "originators = [ 73981,   7535, 692632, 266278,  53849,  95773, 227898,  88022,\n",
    "        62297, 347268,  89700,  36682,  39747,  74843,  98900,  22395,\n",
    "        57513,  82539,  61746,  21398,  87844,  74454,  24255,  86753,\n",
    "        51270,  65821,  66458,  52927,  28777,  34799,  80607,   3538,\n",
    "        36794,  35213,  28732,  34646, 898287,  70392,  22948,  24273,\n",
    "       454545,  85166,  93729,  26266, 692484]\n",
    "\n",
    "ORIGINATORS = [str(o) for o in originators]\n",
    "#ORIGINATORS = [\"24273\", \"61746\"]\n",
    "INPUT = \"./2025-11-13_on_boarding_messages/2025-11-13/com.att.aamg.kafka.prod.sms-prod-eastus2-mt-default.csv\"\n",
    "OUTPUT_DIR = \"./data/input\"\n",
    "MAX_ROWS = 1000000\n",
    "TARGET_ROWS = 5000\n",
    "\n",
    "def normalize(s):\n",
    "    s = s.strip()\n",
    "    if len(s) >= 2 and s[0] == '\"' and s[-1] == '\"':\n",
    "        s = s[1:-1]\n",
    "    return s.replace('\"\"', '\"').replace('\\r', ' ').replace('\\n', ' ').replace('\\t', ' ')\n",
    "\n",
    "def get_source_and_fields(raw):\n",
    "    s = normalize(raw)\n",
    "    try:\n",
    "        o = json.loads(s)\n",
    "    except json.JSONDecodeError:\n",
    "        src = re.search(r'\"sourceAddress\"\\s*:\\s*\"([^\"]*)\"', s)\n",
    "        req = re.search(r'\"requestID\"\\s*:\\s*\"([^\"]*)\"', s)\n",
    "        msg = re.search(r'\"shortMessage\"\\s*:\\s*\"([^\"]*)\"', s)\n",
    "        return (src.group(1) if src else None,\n",
    "                req.group(1) if req else None,\n",
    "                msg.group(1).replace('\\\\\"','\"') if msg else None)\n",
    "    return (o.get('sourceAddress'), o.get('requestID'), o.get('shortMessage'))\n",
    "\n",
    "# First pass: count matches per originator\n",
    "counts = defaultdict(int)\n",
    "with open(INPUT, newline='', encoding='utf-8') as fin:\n",
    "    reader = csv.DictReader(fin)\n",
    "    for i, row in enumerate(reader, 1):\n",
    "        src, _, _ = get_source_and_fields(row['Value'])\n",
    "        if src in ORIGINATORS:\n",
    "            counts[src] += 1\n",
    "        if i >= MAX_ROWS:\n",
    "            break\n",
    "\n",
    "# Compute dilute factors\n",
    "dilute = {}\n",
    "for ori in ORIGINATORS:\n",
    "    c = counts.get(ori, 0)\n",
    "    dilute[ori] = None if c == 0 else max(1, ceil(c / TARGET_ROWS))\n",
    "\n",
    "print(\"Counts:\", dict(counts))\n",
    "print(\"Dilute factors:\", dilute)\n",
    "\n",
    "# Second pass: loop per originator (simpler, one input scan per originator)\n",
    "for ori in ORIGINATORS:\n",
    "    df = dilute.get(ori)\n",
    "    if df is None:\n",
    "        print(f\"No rows for originator {ori}; skipping.\")\n",
    "        continue\n",
    "    outpath = f\"{OUTPUT_DIR}/ref_{ori}.csv\"\n",
    "    with open(outpath, 'w', newline='', encoding='utf-8') as fout, \\\n",
    "         open(INPUT, newline='', encoding='utf-8') as fin:\n",
    "        reader = csv.DictReader(fin)\n",
    "        writer = csv.DictWriter(fout, fieldnames=['originator_id','message_id','raw_text'])\n",
    "        writer.writeheader()\n",
    "        match_counter = 0\n",
    "        for i, row in enumerate(reader, 1):\n",
    "            src, req, msg = get_source_and_fields(row['Value'])\n",
    "            if src == ori:\n",
    "                match_counter += 1\n",
    "                if match_counter % df == 0:\n",
    "                    writer.writerow({'originator_id': src, 'message_id': req, 'raw_text': msg})\n",
    "            if i >= MAX_ROWS:\n",
    "                break\n",
    "    print(f\"Wrote {ori} -> {outpath} (count={counts.get(ori,0)}, df={df})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
