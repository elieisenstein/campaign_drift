{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "817e68e6",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d612cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sms_norm import normalize_and_hash_series, dedupe_by_hash\n",
    "from sms_embed import embed_dedup_dataframe, save_embeddings, load_embeddings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import umap.umap_ as umap\n",
    "import hdbscan\n",
    "from plots import *\n",
    "from sklearn.preprocessing import normalize\n",
    "import joblib\n",
    "import importlib\n",
    "import os\n",
    "import umap\n",
    "import hdbscan\n",
    "from plots import plot_campaigns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e95f38",
   "metadata": {},
   "source": [
    "# Stage 1. Reference Profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336c21e3",
   "metadata": {},
   "source": [
    "## Normalize and embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2879aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./artifacts\\week_synth.csv ./artifacts\\week_synth.npy\n"
     ]
    }
   ],
   "source": [
    "# Load the ready-made synthetic data\n",
    "df_raw = pd.read_csv(\"./artifacts/synthetic_one_originator.csv\")  \n",
    "\n",
    "# Normalize + dedupe\n",
    "norm = normalize_and_hash_series(df_raw[\"raw_text\"], seed=0)\n",
    "norm.insert(0, \"originator_id\", df_raw[\"originator_id\"])\n",
    "norm.insert(1, \"message_id\", df_raw[\"message_id\"])\n",
    "dedup_df, _ = dedupe_by_hash(norm)\n",
    "\n",
    "# Embed (offline MiniLM) and save CSV+NPY\n",
    "LOCAL_MODEL = r\"C:/models/all-MiniLM-L6-v2\"  # local model folder\n",
    "\n",
    "meta_df, X = embed_dedup_dataframe(\n",
    "    dedup_df, text_col=\"normalized_text\", id_col=\"template_hash_xx64\",\n",
    "    batch_size=64, normalize=True, model_name=LOCAL_MODEL\n",
    ")\n",
    "csv_path, npy_path = save_embeddings(meta_df, X, out_dir=\"./artifacts\", prefix=\"week_synth\")\n",
    "print(\"Saved:\", csv_path, npy_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb0e891",
   "metadata": {},
   "source": [
    "## UMAP+HDBSCAN, save cetroids + exemplars (LLM for campaign name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6f8f53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dotenv load error: No module named 'dotenv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eliei\\AppData\\Roaming\\Python\\Python313\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "C:\\Users\\eliei\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eliei\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\eliei\\AppData\\Local\\Temp\\ipykernel_148388\\873737147.py:114: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  dr_start = dr_end = datetime.utcnow().isoformat()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster counts:\n",
      "  C -1:     3\n",
      "  C  0:     5\n",
      "  C  1:     6\n",
      "  C  2:     6\n",
      "  C  3:     6\n",
      "  C  4:    11\n",
      "Saved artifacts:\n",
      " - campaigns_csv: ./artifacts\\week_synth_campaigns.csv\n",
      " - centroids_npy: ./artifacts\\week_synth_campaign_centroids.npy\n",
      " - campaign_examples_csv: ./artifacts\\week_synth_campaign_examples.csv\n",
      " - points_csv: ./artifacts\\week_synth_points.csv\n",
      "Reference build complete. Artifacts saved under: ./artifacts\n"
     ]
    }
   ],
   "source": [
    "# --- Cell: build reference, compute centroids, nearest samples, call LLM, persist ---\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import umap\n",
    "import hdbscan\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict\n",
    "\n",
    "\n",
    "ART_DIR   = \"./artifacts\"\n",
    "PREFIX    = \"week_synth\"  # from save_embeddings(...)\n",
    "META_CSV  = os.path.join(ART_DIR, f\"{PREFIX}.csv\")\n",
    "VEC_NPY   = os.path.join(ART_DIR, f\"{PREFIX}.npy\")\n",
    "\n",
    "assert os.path.exists(META_CSV) and os.path.exists(VEC_NPY), \"Run Stage 4 first to create CSV+NPY.\"\n",
    "\n",
    "# local helpers (write these files next to the notebook)\n",
    "from llm_client import summarize_samples\n",
    "from persist_utils import save_campaign_footprint\n",
    "\n",
    "# PARAMETERS\n",
    "N_NEAREST = 5              # take top K = min(N_NEAREST, cluster_size)\n",
    "UMAP_PARAMS: Dict[str, Any] = dict(\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.1,\n",
    "    metric=\"cosine\",\n",
    "    random_state=42,\n",
    "    force_approximation_algorithm=False,\n",
    "    transform_seed=42,\n",
    ")\n",
    "\n",
    "HDBSCAN_PARAMS: Dict[str, Any] = dict(\n",
    "    min_cluster_size=5,\n",
    "    min_samples=2,\n",
    "    metric=\"euclidean\",\n",
    ")\n",
    "\n",
    "# Load meta + embeddings\n",
    "meta = pd.read_csv(META_CSV, encoding=\"utf-8\")\n",
    "X = np.load(VEC_NPY)  # shape [M, D]\n",
    "\n",
    "# Safety: ensure unit-norm (cosine-ready)\n",
    "X = X.astype(np.float32)\n",
    "X = X / np.maximum(np.linalg.norm(X, axis=1, keepdims=True), 1e-12)\n",
    "\n",
    "# UMAP to 2D (for visualization only)\n",
    "umap_model = umap.UMAP(**UMAP_PARAMS)\n",
    "X_2d = umap_model.fit_transform(X)\n",
    "joblib.dump(umap_model, os.path.join(ART_DIR, \"umap_model.pkl\"))\n",
    "\n",
    "# HDBSCAN clustering\n",
    "clusterer = hdbscan.HDBSCAN(**HDBSCAN_PARAMS)\n",
    "labels = clusterer.fit_predict(X)\n",
    "\n",
    "# attach labels into meta so saved points include label column\n",
    "meta = meta.copy()\n",
    "meta[\"label\"] = labels\n",
    "\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print(\"Cluster counts:\")\n",
    "for k, v in zip(unique, counts):\n",
    "    print(f\"  C{k:>3}: {v:>5d}\")\n",
    "\n",
    "# Compute centroids and select exemplars (nearest samples)\n",
    "cluster_labels = sorted([c for c in set(labels) if c != -1])\n",
    "centroids = []\n",
    "campaign_rows = []\n",
    "examples_rows = []  # will hold campaign_examples (one row per exemplar)\n",
    "for row_idx, cl in enumerate(cluster_labels):\n",
    "    member_idx = np.where(labels == cl)[0]\n",
    "    if member_idx.size == 0:\n",
    "        continue\n",
    "    # centroid in original embedding space (384-D), unit-norm\n",
    "    cvec = X[member_idx].mean(axis=0)\n",
    "    cvec = cvec / (np.linalg.norm(cvec) + 1e-12)\n",
    "    centroids.append(cvec.astype(np.float32))\n",
    "\n",
    "    # sims & ordering (cosine because unit-norm)\n",
    "    sims = X[member_idx] @ cvec\n",
    "    order = np.argsort(-sims)\n",
    "    ordered_idx = member_idx[order]\n",
    "\n",
    "    k = min(N_NEAREST, ordered_idx.size)\n",
    "    top_idx = ordered_idx[:k]\n",
    "\n",
    "    # collect exemplars rows\n",
    "    for rank, global_i in enumerate(top_idx, start=1):\n",
    "        sim_value = float(sims[order][rank-1])  # matches rank order\n",
    "        examples_rows.append({\n",
    "            \"campaign_row_index\": row_idx,\n",
    "            \"campaign_label\": int(cl),\n",
    "            \"rank\": int(rank),\n",
    "            \"message_id\": meta.iloc[global_i].get(\"message_id\", \"\"),\n",
    "            \"template_hash_xx64\": meta.iloc[global_i].get(\"template_hash_xx64\", \"\"),\n",
    "            # Prefer normalized_text (masked). Use raw_text only if you understand privacy/PII.\n",
    "            \"text_sample\": meta.iloc[global_i].get(\"normalized_text\", meta.iloc[global_i].get(\"raw_text\", \"\")),\n",
    "            \"sim_score\": sim_value,\n",
    "            \"count_in_window\": int(meta.iloc[global_i].get(\"count_in_window\", 1))\n",
    "        })\n",
    "\n",
    "    # campaign-level metadata\n",
    "    proto_count = int(member_idx.size)\n",
    "    msg_count = int(meta.iloc[member_idx][\"count_in_window\"].sum()) if \"count_in_window\" in meta.columns else proto_count\n",
    "    # date range: try to use timestamp fields if present else use current build date\n",
    "    if {\"window_start\", \"window_end\"}.issubset(meta.columns):\n",
    "        dr_start = meta.iloc[member_idx][\"window_start\"].min()\n",
    "        dr_end = meta.iloc[member_idx][\"window_end\"].max()\n",
    "    elif {\"timestamp\"}.issubset(meta.columns):\n",
    "        dr_start = meta.iloc[member_idx][\"timestamp\"].min()\n",
    "        dr_end = meta.iloc[member_idx][\"timestamp\"].max()\n",
    "    else:\n",
    "        dr_start = dr_end = datetime.utcnow().isoformat()\n",
    "\n",
    "    campaign_rows.append({\n",
    "        \"row_index\": row_idx,                   # index into centroids array\n",
    "        \"cluster_label\": int(cl),               # original HDBSCAN label\n",
    "        \"proto_count\": proto_count,\n",
    "        \"msg_count\": msg_count,\n",
    "        \"date_range_start\": dr_start,\n",
    "        \"date_range_end\": dr_end,\n",
    "        \"campaign_name\": \"\",                    # filled later by LLM/human\n",
    "        \"status\": \"Known\"\n",
    "    })\n",
    "\n",
    "# Stack centroids\n",
    "if len(centroids) > 0:\n",
    "    C = np.vstack(centroids)\n",
    "else:\n",
    "    C = np.zeros((0, X.shape[1]), dtype=np.float32)\n",
    "\n",
    "# Call LLM summarizer for each campaign using the nearest samples (deterministic)\n",
    "campaigns_df = pd.DataFrame(campaign_rows)\n",
    "examples_df = pd.DataFrame(examples_rows)\n",
    "\n",
    "# For each campaign_row_index gather sorted samples and call LLM\n",
    "campaign_names = []\n",
    "for row in campaigns_df.itertuples(index=False):\n",
    "    row_idx = int(row.row_index) # type: ignore\n",
    "    samples = examples_df[examples_df[\"campaign_row_index\"] == row_idx].sort_values(\"rank\")[\"text_sample\"].tolist()\n",
    "    # Ensure samples are masked/normalized. summarize_samples will double-check.\n",
    "    if len(samples) == 0:\n",
    "        summary = \"\"\n",
    "        raw_resp = None\n",
    "    else:\n",
    "        # Use the llm_client default model (gpt-4o-mini) or specify explicitly\n",
    "        summary, raw_resp = summarize_samples(samples, max_words=5, model=\"gpt-4o-mini\", temperature=0.0)\n",
    "    campaign_names.append(summary or \"\")\n",
    "\n",
    "campaigns_df[\"campaign_name\"] = campaign_names\n",
    "\n",
    "# Persist everything using helper (creates campaigns.csv, campaigns.npy, campaign_examples.csv, points.csv)\n",
    "save_campaign_footprint(\n",
    "    out_dir=ART_DIR,\n",
    "    prefix=PREFIX,\n",
    "    campaigns_df=campaigns_df,\n",
    "    centroids=C,\n",
    "    campaign_examples_df=examples_df,\n",
    "    points_meta=meta,\n",
    "    points_2d=X_2d, # type: ignore\n",
    ")\n",
    "\n",
    "print(\"Reference build complete. Artifacts saved under:\", ART_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ecf1f0",
   "metadata": {},
   "source": [
    "## Plot UMAP of reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c35341",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_path     = os.path.join(ART_DIR, \"umap_hdbscan_campaigns.png\")\n",
    "cluster_ids = campaigns_df[\"cluster_label\"].tolist()\n",
    "plot_campaigns(X_2d, labels, cluster_ids, plot_path)\n",
    "print(\" - plot          :\", plot_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d08c124",
   "metadata": {},
   "source": [
    "# Stage 2. New data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4250d133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New batch rows = 200; dedup prototypes = 37\n",
      "Dedup prototypes in first slice (<=160 rows): 37\n",
      "Dedup prototypes in last slice (~new campaign): 0\n",
      "\n",
      "Prototype-level assignment (status counts):\n",
      "status\n",
      "Known      26\n",
      "Unknown    11\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique prototype counts by status:\n",
      "    status  unique_prototypes\n",
      "0    Known                 26\n",
      "1  Unknown                 11\n",
      "\n",
      "Overlap with reference prototypes: 37 prototypes present in new prototypes\n",
      "Sample of new normalized_texts NOT in reference (up to 10):\n",
      "[]\n",
      "\n",
      "Status counts in first 37 prototypes (expected original):\n",
      "status\n",
      "Known      26\n",
      "Unknown    11\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Status counts in last 0 prototypes (expected new campaign prototypes):\n",
      "Series([], Name: count, dtype: int64)\n",
      "\n",
      "Saved prototype assignments to: ./artifacts\\week_synth_new_prototypes_assignments.csv\n",
      "Saved prototype embeddings to: ./artifacts\\week_synth_new_prototypes.npy\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Stage 2: assign new batch to saved campaign centroids ----------------\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# exact Stage-1 helpers (must exist)\n",
    "from sms_norm import normalize_and_hash_series, dedupe_by_hash\n",
    "from sms_embed import embed_dedup_dataframe\n",
    "\n",
    "ART_DIR   = \"./artifacts\"\n",
    "PREFIX    = \"week_synth\"\n",
    "#NEW_BATCH = \"./artifacts/mixed_160_orig__40_new.csv\"  \n",
    "NEW_BATCH = \"./artifacts/synthetic_one_originator.csv\"\n",
    "SIM_THRESHOLD = 0.8  # per SOW\n",
    "\n",
    "# --- validate artifacts ---\n",
    "centroids_path = os.path.join(ART_DIR, f\"{PREFIX}_campaign_centroids.npy\")\n",
    "campaigns_csv   = os.path.join(ART_DIR, f\"{PREFIX}_campaigns.csv\")\n",
    "meta_csv        = os.path.join(ART_DIR, f\"{PREFIX}.csv\")   # Stage-1 prototypes meta\n",
    "\n",
    "assert os.path.exists(centroids_path), f\"Missing centroids: {centroids_path}\"\n",
    "assert os.path.exists(campaigns_csv), f\"Missing campaigns CSV: {campaigns_csv}\"\n",
    "assert os.path.exists(NEW_BATCH), f\"Missing new batch: {NEW_BATCH}\"\n",
    "assert os.path.exists(meta_csv), f\"Missing reference meta CSV: {meta_csv}\"\n",
    "\n",
    "centroids = np.load(centroids_path).astype(np.float32)  # [K, D]\n",
    "campaigns_df = pd.read_csv(campaigns_csv)\n",
    "ref_meta_df  = pd.read_csv(meta_csv, encoding=\"utf-8\")\n",
    "\n",
    "# normalize centroids for cosine\n",
    "centroids = centroids / np.maximum(np.linalg.norm(centroids, axis=1, keepdims=True), 1e-12)\n",
    "\n",
    "# --- load new batch and produce deduplicated prototypes (exactly as Stage-1) ---\n",
    "df_new = pd.read_csv(NEW_BATCH, encoding=\"utf-8\")\n",
    "\n",
    "# Choose raw text column (Stage-1 used \"raw_text\")\n",
    "if \"raw_text\" in df_new.columns:\n",
    "    raw_col = \"raw_text\"\n",
    "elif \"text\" in df_new.columns:\n",
    "    raw_col = \"text\"\n",
    "else:\n",
    "    raw_col = [c for c in df_new.columns if df_new[c].dtype == object][0]\n",
    "\n",
    "# Normalize + compute template hashes (identical function used in Stage-1)\n",
    "norm_df = normalize_and_hash_series(df_new[raw_col].astype(str), seed=0)\n",
    "\n",
    "# Preserve identifiers if present\n",
    "if \"message_id\" in df_new.columns:\n",
    "    norm_df.insert(0, \"message_id\", df_new[\"message_id\"].values) # type: ignore\n",
    "if \"originator_id\" in df_new.columns and \"originator_id\" not in norm_df.columns:\n",
    "    norm_df.insert(0, \"originator_id\", df_new[\"originator_id\"].values) # type: ignore\n",
    "\n",
    "# Deduplicate by hash -> dedup_df is prototypes (one row per unique normalized_text)\n",
    "dedup_df, _ = dedupe_by_hash(norm_df)\n",
    "\n",
    "# Diagnostics about dedup\n",
    "n_total_rows = len(df_new)\n",
    "n_prototypes = len(dedup_df)\n",
    "print(f\"New batch rows = {n_total_rows}; dedup prototypes = {n_prototypes}\")\n",
    "\n",
    "# Also show dedup counts inside the first / last slices (prototype counts)\n",
    "first_slice_count = dedup_df.iloc[:min(160, n_total_rows)]['normalized_text'].nunique()\n",
    "last_slice_count  = dedup_df.iloc[min(160, n_total_rows):]['normalized_text'].nunique()\n",
    "print(f\"Dedup prototypes in first slice (<=160 rows): {first_slice_count}\")\n",
    "print(f\"Dedup prototypes in last slice (~new campaign): {last_slice_count}\")\n",
    "\n",
    "# --- Embed prototypes only (same function used in Stage-1) ---\n",
    "# embed_dedup_dataframe returns (meta_df, X) where meta_df contains 'template_hash_xx64' and 'normalized_text'\n",
    "meta_df, X_proto = embed_dedup_dataframe(\n",
    "    dedup_df,\n",
    "    text_col=\"normalized_text\",\n",
    "    id_col=\"template_hash_xx64\",\n",
    "    batch_size=64,\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "# Ensure dtype and normalize\n",
    "X_proto = np.asarray(X_proto, dtype=np.float32)\n",
    "X_proto = X_proto / np.maximum(np.linalg.norm(X_proto, axis=1, keepdims=True), 1e-12)\n",
    "\n",
    "# Save dedup prototype embeddings & meta for traceability (optional)\n",
    "out_meta_prototypes = os.path.join(ART_DIR, f\"{PREFIX}_new_prototypes.csv\")\n",
    "out_npy_prototypes  = os.path.join(ART_DIR, f\"{PREFIX}_new_prototypes.npy\")\n",
    "meta_df.to_csv(out_meta_prototypes, index=False, encoding=\"utf-8\")\n",
    "np.save(out_npy_prototypes, X_proto.astype(np.float32))\n",
    "\n",
    "# --- Assign prototypes to nearest campaign centroids (prototype-level assignment only) ---\n",
    "sims_proto = X_proto @ centroids.T     # [P, K]\n",
    "assigned_proto_idx = np.argmax(sims_proto, axis=1)\n",
    "assigned_proto_score = np.max(sims_proto, axis=1)\n",
    "proto_status = np.where(assigned_proto_score >= SIM_THRESHOLD, \"Known\", \"Unknown\")\n",
    "\n",
    "# Attach assignment results to meta_df (prototype-level output)\n",
    "meta_df[\"assigned_campaign_row_index\"] = assigned_proto_idx\n",
    "meta_df[\"assigned_campaign_score\"] = assigned_proto_score\n",
    "meta_df[\"assigned_campaign_label\"] = [campaigns_df.loc[i,\"cluster_label\"] if (i < len(campaigns_df)) else None for i in assigned_proto_idx]\n",
    "meta_df[\"assigned_campaign_name\"] = [campaigns_df.loc[i,\"campaign_name\"] if (i < len(campaigns_df)) else \"\" for i in assigned_proto_idx]\n",
    "meta_df[\"status\"] = proto_status\n",
    "\n",
    "# Save prototype-level assignments \n",
    "proto_out_path = os.path.join(ART_DIR, f\"{PREFIX}_new_prototypes_assignments.csv\")\n",
    "meta_df.to_csv(proto_out_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "# --- Prototype-level diagnostics (what you care about) ---\n",
    "print(\"\\nPrototype-level assignment (status counts):\")\n",
    "print(meta_df[\"status\"].value_counts())\n",
    "\n",
    "print(\"\\nUnique prototype counts by status:\")\n",
    "print(meta_df.groupby(\"status\")[\"template_hash_xx64\"].nunique().reset_index(name=\"unique_prototypes\"))\n",
    "\n",
    "# Overlap with Stage-1 prototypes (ref_meta_df)\n",
    "ref_norms = set(ref_meta_df[\"normalized_text\"].astype(str).unique())\n",
    "new_norms = set(meta_df[\"normalized_text\"].astype(str).unique())\n",
    "overlap = new_norms & ref_norms\n",
    "print(f\"\\nOverlap with reference prototypes: {len(overlap)} prototypes present in new prototypes\")\n",
    "print(\"Sample of new normalized_texts NOT in reference (up to 10):\")\n",
    "print(list(new_norms - ref_norms)[:10])\n",
    "\n",
    "# Status counts in first / last prototype slices (prototype-space)\n",
    "P = len(meta_df)\n",
    "first_p_slice = meta_df.iloc[:min(160, P)]\n",
    "last_p_slice  = meta_df.iloc[min(160, P):]\n",
    "print(f\"\\nStatus counts in first {len(first_p_slice)} prototypes (expected original):\")\n",
    "print(first_p_slice[\"status\"].value_counts())\n",
    "\n",
    "print(f\"\\nStatus counts in last {len(last_p_slice)} prototypes (expected new campaign prototypes):\")\n",
    "print(last_p_slice[\"status\"].value_counts())\n",
    "\n",
    "print(f\"\\nSaved prototype assignments to: {proto_out_path}\")\n",
    "print(f\"Saved prototype embeddings to: {out_npy_prototypes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3c2853",
   "metadata": {},
   "source": [
    "# debug plot of new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a72abc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug plot (aligned with Stage 5 UMAP) ===\n",
    "# - Uses saved reducer (umap_model.pkl) with deterministic transform\n",
    "# - Stars = mean of saved 2D coords (points.csv)\n",
    "# - New points: colored by assigned centroid's LABEL; noise in gray\n",
    "\n",
    "\n",
    "# load artifacts\n",
    "umap_model = joblib.load(\"./artifacts/umap_model.pkl\")          # same reducer from Stage 5\n",
    "points_df  = pd.read_csv(\"./artifacts/points.csv\")              # has 'umap_x','umap_y','label'\n",
    "X_ref      = np.load(\"./artifacts/week_synth.npy\")              # optional (not used if X_ref_2d passed)\n",
    "X_ref_2d   = points_df[[\"umap_x\",\"umap_y\"]].to_numpy()\n",
    "ref_labels = points_df[\"label\"].to_numpy()\n",
    "\n",
    "# row -> label mapping (used to color new points by campaign label)\n",
    "cmap_df = pd.read_csv(\"./artifacts/campaigns.csv\").sort_values(\"row_index\")\n",
    "centroid_labels = cmap_df[\"cluster_label\"].to_numpy(dtype=int)\n",
    "\n",
    "# plotting function (reloaded to ensure latest version with X_ref_2d=)\n",
    "import plots\n",
    "importlib.reload(plots)\n",
    "from plots import plot_ref_stars_mean2d_with_new\n",
    "\n",
    "# call the plotter (reuses a_idx, noise_mask, C, X from Cell 1)\n",
    "plot_ref_stars_mean2d_with_new(\n",
    "    reducer=umap_model,\n",
    "    X_ref=X_ref,                 # not used if X_ref_2d provided; kept for API symmetry\n",
    "    ref_labels=ref_labels,\n",
    "    C=C,\n",
    "    X_new=X,\n",
    "    a_idx=a_idx,\n",
    "    noise_mask=noise_mask,\n",
    "    centroid_labels=centroid_labels,\n",
    "    X_ref_2d=X_ref_2d,           # ensures stars match Stage-5 exactly\n",
    "    title=\"Stage 6 Debug: New vs Reference (aligned UMAP)\",\n",
    "    save_path=\"./artifacts/stage6_debug_umap.png\",\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
